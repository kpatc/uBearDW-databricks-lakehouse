# Architecture uBear Eats Data Warehouse - Databricks Lakehouse

## ðŸŽ¯ Vue d'ensemble

Ce document dÃ©crit l'architecture complÃ¨te du Data Warehouse uBear Eats construit sur Databricks avec une approche moderne de Lakehouse utilisant Delta Lake et l'architecture Medallion.

## ðŸ“Š Architecture Medallion (Bronze â†’ Silver â†’ Gold)

### Bronze Layer (Raw Data)
**Objectif**: Ingestion brute des donnÃ©es CDC en temps rÃ©el

- **Source**: PostgreSQL (eater, merchant, courier, trip_events)
- **CDC**: Debezium â†’ Kafka â†’ Databricks
- **Pipeline**: Delta Live Tables (DLT) Streaming
- **Format**: Delta Lake avec Change Data Feed
- **Fichier**: `pipelines/bronze_pipeline.py`

**Tables**:
```
- trip_events_bronze (Ã©vÃ©nements commandes/livraisons)
- eater_bronze (clients)
- merchant_bronze (restaurants)
- courier_bronze (livreurs)
```

**CaractÃ©ristiques**:
- Streaming continu (mode continuous)
- Watermark 10 minutes sur event_time
- Preservation de l'enveloppe Debezium CDC
- Validation clÃ©s primaires (NOT NULL)

### Silver Layer (Cleaned Data)
**Objectif**: Nettoyage, validation et structuration des donnÃ©es

- **Source**: Tables Bronze
- **Pipeline**: Delta Live Tables (DLT) Streaming
- **Format**: Delta Lake avec Change Data Feed
- **Fichier**: `pipelines/silver_pipeline.py`

**Transformations appliquÃ©es**:
1. **Parsing JSON**: Extraction du payload Debezium
2. **Nettoyage**:
   - Normalisation emails (uppercase, trim)
   - Normalisation adresses (trim, postal code cleanup)
   - Normalisation plaques d'immatriculation
3. **Validation**: DLT Expectations pour qualitÃ©
4. **DÃ©duplication**: Sur clÃ©s mÃ©tier
5. **Enrichissement**: Calcul partitions (date_partition)

**Tables**:
```
- trip_events_silver (Ã©vÃ©nements nettoyÃ©s avec payload parsÃ©)
- eater_silver (clients validÃ©s)
- merchant_silver (restaurants nettoyÃ©s)
- courier_silver (livreurs validÃ©s)
```

**Expectations de qualitÃ©**:
- Emails valides (format, NOT NULL)
- Montants positifs (â‰¥ 0)
- Ratings dans plage valide (1-5)
- Distances raisonnables (< 100 miles)

### Gold Layer (Analytics Ready)
**Objectif**: ModÃ¨le dimensionnel pour analytics (Star Schema)

- **Source**: Tables Silver
- **Pipeline**: Notebook PySpark Batch
- **Format**: Delta Lake optimisÃ© (Z-Order)
- **Fichier**: `pipelines/gold_pipeline.py`
- **ExÃ©cution**: Job batch quotidien (2 AM UTC)

**Dimensions SCD Type 2** (historisation complÃ¨te):
```
- dim_eater (clients avec historique)
  â””â”€ Colonnes SCD2: effective_start_date, effective_end_date, 
     is_current, version_number, row_hash
  
- dim_merchant (restaurants avec historique)
  â””â”€ PartitionnÃ© par: city
  
- dim_courier (livreurs avec historique)
```

**Dimensions statiques**:
```
- dim_date (calendrier 2020-2030)
  â””â”€ Colonnes: date_key, full_date, day_of_week, is_weekend, 
     is_holiday, week_of_year, month, quarter, year
  
- dim_time (1440 minutes par jour)
  â””â”€ Colonnes: time_key, hour_24, hour_12, am_pm, time_period, 
     is_peak_hour
  
- dim_location (gÃ©ographie - Ã  implÃ©menter)
  â””â”€ Colonnes: latitude, longitude, geohash, h3_index, region_zone
```

**Table de faits**:
```
- trip_fact (commandes et livraisons)
  â””â”€ ClÃ©: trip_id (order_id)
  â””â”€ Mesures: montants, mÃ©triques temps, ratings
  â””â”€ PartitionnÃ© par: date_partition, region_partition
  â””â”€ OptimisÃ©: Z-Order sur (trip_id, eater_id, merchant_id, order_placed_at)
  â””â”€ MERGE upsert sur trip_id basÃ© sur updated_at
```

## ðŸ”„ Flux de donnÃ©es

```
PostgreSQL (OLTP)
    â”‚
    â–¼
Debezium CDC (WAL)
    â”‚
    â–¼
Kafka Topics
    â”‚
    â”œâ”€ dbserver1.public.trip_events
    â”œâ”€ dbserver1.public.eater
    â”œâ”€ dbserver1.public.merchant
    â””â”€ dbserver1.public.courier
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER (Streaming DLT)       â”‚
â”‚  - trip_events_bronze               â”‚
â”‚  - eater_bronze                     â”‚
â”‚  - merchant_bronze                  â”‚
â”‚  - courier_bronze                   â”‚
â”‚  Mode: Continuous Streaming         â”‚
â”‚  Latency: < 1 minute                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER (Streaming DLT)       â”‚
â”‚  - trip_events_silver               â”‚
â”‚  - eater_silver                     â”‚
â”‚  - merchant_silver                  â”‚
â”‚  - courier_silver                   â”‚
â”‚  Mode: Continuous Streaming         â”‚
â”‚  Latency: < 2 minutes               â”‚
â”‚  Quality: DLT Expectations          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER (Batch Notebook)        â”‚
â”‚  DIMENSIONS (SCD2):                 â”‚
â”‚    - dim_eater, dim_merchant        â”‚
â”‚    - dim_courier                    â”‚
â”‚  DIMENSIONS (Static):               â”‚
â”‚    - dim_date, dim_time             â”‚
â”‚  FACTS:                             â”‚
â”‚    - trip_fact (MERGE upsert)       â”‚
â”‚  Schedule: Daily 2 AM UTC           â”‚
â”‚  Duration: ~30-60 minutes           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
      BI Tools & Analytics
      (Tableau, Power BI, SQL)
```

## ðŸ› ï¸ Orchestration

### Jobs Databricks

#### 1. Streaming Job (Continu)
**Fichier**: `jobs/streaming_job.json`

- **Pipeline Bronze**: Ingestion CDC Kafka â†’ Bronze
- **Pipeline Silver**: Transformation Bronze â†’ Silver
- **Mode**: Continuous (24/7)
- **DÃ©pendances**: Bronze doit rÃ©ussir avant Silver

#### 2. Batch Job (Quotidien)
**Fichier**: `jobs/batch_job.json`

- **Task 1**: Gold Dimensions SCD2 + Trip Fact
- **Task 2**: Optimize Tables (OPTIMIZE, Z-ORDER)
- **Task 3**: Data Quality Checks
- **Schedule**: Cron `0 0 2 * * ?` (2 AM UTC daily)
- **Cluster**: Job Cluster (spot instances avec fallback)

## ðŸ“ Patterns et Best Practices

### 1. SCD Type 2 Implementation

```python
# Logique SCD2 gÃ©nÃ©rique (utils/transformations.py)
def apply_scd2_merge(source_df, target_table, business_keys, compare_columns):
    # 1. Calculer row_hash sur compare_columns
    # 2. Joindre source et target sur business_keys
    # 3. Identifier changements (hash diffÃ©rent)
    # 4. Expirer anciens records (is_current=False, effective_end_date=now)
    # 5. InsÃ©rer nouveaux records (version_number+1, is_current=True)
```

**Avantages**:
- Historisation complÃ¨te des changements
- PossibilitÃ© de requÃªter "as of" une date
- Audit trail complet

### 2. Data Quality avec DLT Expectations

```python
# Bronze: Validation stricte des clÃ©s
@dlt.expect_or_drop("valid_order_id", "order_id IS NOT NULL")

# Silver: Validation mÃ©tier
@dlt.expect_or_drop("valid_email", "email LIKE '%@%'")
@dlt.expect("valid_ratings", "eater_rating BETWEEN 1 AND 5")

# Gold: Validation intÃ©gritÃ© rÃ©fÃ©rentielle
@dlt.expect_or_fail("valid_foreign_keys", "eater_id IS NOT NULL")
```

**Niveaux**:
- `expect()`: Log seulement (mÃ©triques)
- `expect_or_drop()`: Rejette les invalides
- `expect_or_fail()`: Fait Ã©chouer le pipeline

### 3. Partitionnement et Optimisation

**StratÃ©gies de partitionnement**:
```python
# trip_fact: Double partition
.partitionBy("date_partition", "region_partition")

# dim_merchant: Partition par city
.partitionBy("city")

# Z-Order pour queries frÃ©quentes
OPTIMIZE trip_fact ZORDER BY (trip_id, eater_id, merchant_id)
```

**Avantages**:
- Pruning efficace lors des queries
- Performances accrues sur large volume
- CoÃ»ts de stockage optimisÃ©s

### 4. MERGE Upserts pour Idempotence

```python
# trip_fact: Upsert basÃ© sur updated_at
deltaTable.merge(
    source_df,
    "target.trip_id = source.trip_id"
).whenMatchedUpdate(
    condition="source.updated_at > target.updated_at",
    set={...}
).whenNotMatchedInsertAll().execute()
```

**Avantages**:
- Idempotence (replay safe)
- Gestion des late arrivals
- Pas de duplicates

## ðŸ”’ SÃ©curitÃ© et Gouvernance

### Unity Catalog (RecommandÃ©)

```
Catalog: ubear_catalog
â”‚
â”œâ”€ Schema: ubear_bronze
â”‚  â””â”€ Tables: *_bronze
â”‚  â””â”€ Access: Data Engineering (READ/WRITE)
â”‚
â”œâ”€ Schema: ubear_silver
â”‚  â””â”€ Tables: *_silver
â”‚  â””â”€ Access: Data Engineering (READ/WRITE), Analytics (READ)
â”‚
â””â”€ Schema: ubear_gold
   â””â”€ Tables: dim_*, trip_fact
   â””â”€ Access: Analytics (READ), BI Tools (READ)
```

### Row-Level Security (Future)

```sql
-- Example: Restriction par region
CREATE FUNCTION filter_by_region()
RETURN region_partition = current_user_region();

ALTER TABLE trip_fact SET ROW FILTER filter_by_region ON (region_partition);
```

## ðŸ“ˆ Monitoring et ObservabilitÃ©

### MÃ©triques clÃ©s

**Streaming Pipelines (Bronze/Silver)**:
- Input Rate (records/sec)
- Processing Time (latency)
- Data Quality Violations (par expectation)
- Backlog (lag derriÃ¨re Kafka)

**Batch Pipeline (Gold)**:
- Records Inserted/Updated (par table)
- SCD2 Changes Detected
- Job Duration
- Data Freshness (derniÃ¨re mise Ã  jour)

### Alerting

```json
{
  "email_notifications": {
    "on_failure": ["data-team@ubear.com"],
    "on_success": ["data-team@ubear.com"]
  }
}
```

## ðŸš€ Ã‰volutions futures

1. **Near Real-Time Gold**: Remplacer batch par streaming pour trip_fact
2. **ML Features**: Calculer features pour modÃ¨les ML
3. **Aggregate Tables**: Tables prÃ©-agrÃ©gÃ©es pour dashboards
4. **dim_location**: Enrichir avec H3, gÃ©ohash, zones livraison
5. **Data Retention**: Politique de retention avec VACUUM
6. **CDC Multi-Source**: Ajouter d'autres sources (events app mobile, etc.)

## ðŸ“š RÃ©fÃ©rences

- [Databricks Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html)
- [SCD Type 2 avec Delta Lake](https://docs.databricks.com/delta/merge.html)
- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: DÃ©cembre 2025  
**Maintenu par**: Data Engineering Team
