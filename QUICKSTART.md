# Guide de D√©marrage Rapide - uBear DW

Ce guide vous permet de d√©ployer rapidement le Data Warehouse uBear Eats sur Databricks.

## ‚ö° Pr√©requis

- ‚úÖ Workspace Databricks (AWS, Azure ou GCP)
- ‚úÖ Kafka cluster avec topics configur√©s
- ‚úÖ PostgreSQL source avec CDC activ√© (wal_level=logical)
- ‚úÖ Git repository clon√© dans Databricks Repos

## üìù √âtapes de d√©ploiement (30 minutes)

### 1Ô∏è‚É£ Configuration du Workspace (5 min)

#### a) Cr√©er le catalogue et les sch√©mas

```sql
-- Dans un notebook SQL Databricks
CREATE CATALOG IF NOT EXISTS ubear_catalog;
USE CATALOG ubear_catalog;

CREATE SCHEMA IF NOT EXISTS ubear_bronze
  COMMENT 'Raw CDC data from source systems';

CREATE SCHEMA IF NOT EXISTS ubear_silver
  COMMENT 'Cleaned and validated data';

CREATE SCHEMA IF NOT EXISTS ubear_gold
  COMMENT 'Analytics-ready dimensional model';
```

#### b) Configurer les secrets (Databricks CLI)

```bash
# Cr√©er scope pour secrets
databricks secrets create-scope --scope ubear-secrets

# Ajouter secrets Kafka
databricks secrets put --scope ubear-secrets --key kafka-bootstrap-servers
# Entrer: kafka.example.com:9092

# Ajouter secrets PostgreSQL (si n√©cessaire)
databricks secrets put --scope ubear-secrets --key postgres-password
```

### 2Ô∏è‚É£ Importer le code dans Repos (2 min)

```bash
# Dans Databricks UI: Workspace ‚Üí Repos ‚Üí Add Repo
Repository URL: https://github.com/kpatc/uBearDW-databricks-lakehouse
Git provider: GitHub
Branch: main
Path: /Repos/ubear-dw
```

### 3Ô∏è‚É£ Cr√©er le Pipeline Bronze (5 min)

#### Option A: Via UI Databricks

1. Aller √† **Delta Live Tables** ‚Üí **Create Pipeline**
2. Remplir:
   - **Name**: `ubear_bronze_streaming`
   - **Product Edition**: Advanced
   - **Notebook libraries**: `/Repos/ubear-dw/pipelines/bronze_pipeline`
   - **Storage location**: `/mnt/datalake/ubear/dlt/bronze`
   - **Target schema**: `ubear_bronze`
3. Configuration (sous Advanced):
   ```json
   {
     "kafka.bootstrap.servers": "{{secrets/ubear-secrets/kafka-bootstrap-servers}}"
   }
   ```
4. Cluster: Autoscaling 2-8 workers
5. **Pipeline mode**: Continuous
6. Cliquer **Create**

#### Option B: Via API/CLI

```bash
databricks pipelines create --json-file jobs/bronze_pipeline_config.json
```

### 4Ô∏è‚É£ Cr√©er le Pipeline Silver (5 min)

R√©p√©ter les √©tapes ci-dessus avec:
- **Name**: `ubear_silver_streaming`
- **Notebook**: `/Repos/ubear-dw/pipelines/silver_pipeline`
- **Storage**: `/mnt/datalake/ubear/dlt/silver`
- **Target**: `ubear_silver`
- **Pipeline mode**: Continuous

### 5Ô∏è‚É£ Cr√©er les tables Gold (3 min)

```sql
-- Ex√©cuter le script DDL dans un notebook SQL
%sql
-- Copier le contenu de databricks_setup/02_create_tables.sql
-- Ou utiliser le notebook gold_pipeline qui cr√©e les tables automatiquement
```

### 6Ô∏è‚É£ Cr√©er le Job Batch Gold (5 min)

1. Aller √† **Workflows** ‚Üí **Create Job**
2. Importer la configuration:
   ```bash
   # Via CLI
   databricks jobs create --json-file jobs/batch_job.json
   ```
3. Ou cr√©er manuellement:
   - **Name**: `uBear_DW_Batch_Gold_Processing`
   - **Task 1**: Notebook `/Repos/ubear-dw/pipelines/gold_pipeline`
   - **Cluster**: Job cluster (4 workers i3.xlarge)
   - **Schedule**: Cron `0 0 2 * * ?` (2 AM UTC)

### 7Ô∏è‚É£ D√©marrer les Pipelines (2 min)

```bash
# D√©marrer Bronze (via UI ou CLI)
databricks pipelines start --pipeline-id <bronze_pipeline_id>

# D√©marrer Silver
databricks pipelines start --pipeline-id <silver_pipeline_id>

# Le job Gold est schedul√© et d√©marrera automatiquement
```

### 8Ô∏è‚É£ V√©rifier le flux de donn√©es (3 min)

```sql
-- V√©rifier Bronze
SELECT COUNT(*) FROM ubear_bronze.trip_events_bronze;
SELECT COUNT(*) FROM ubear_bronze.eater_bronze;

-- V√©rifier Silver (attendre 1-2 min)
SELECT COUNT(*) FROM ubear_silver.trip_events_silver;
SELECT COUNT(*) FROM ubear_silver.eater_silver;

-- V√©rifier Gold (apr√®s ex√©cution du job)
SELECT COUNT(*) FROM ubear_gold.dim_eater;
SELECT COUNT(*) FROM ubear_gold.trip_fact;
```

## üß™ Test avec donn√©es locales

Si vous voulez tester avec l'environnement local Docker:

```bash
# Depuis le r√©pertoire local_stack/
cd local_stack

# D√©marrer les services
docker-compose up -d

# Attendre 30 secondes
sleep 30

# G√©n√©rer des donn√©es
./generate_data.sh

# Enregistrer le connecteur Debezium
./register_connector.sh

# V√©rifier que les topics Kafka sont cr√©√©s
docker exec -it local_stack-kafka-1 kafka-topics --list --bootstrap-server localhost:9092
```

Ensuite, configurer Databricks pour pointer vers votre Kafka local (ou utiliser un tunnel).

## üîç Monitoring

### V√©rifier la sant√© des pipelines

**Bronze/Silver DLT**:
```
Databricks UI ‚Üí Delta Live Tables ‚Üí S√©lectionner pipeline ‚Üí Data Quality Tab
```

M√©triques cl√©s:
- ‚úÖ Records processed
- ‚ö†Ô∏è Expectations violations
- ‚ùå Pipeline failures

**Gold Batch Job**:
```
Databricks UI ‚Üí Workflows ‚Üí S√©lectionner job ‚Üí Runs
```

V√©rifier:
- ‚úÖ Derni√®re ex√©cution r√©ussie
- ‚è±Ô∏è Dur√©e d'ex√©cution
- üìä Records inserted/updated

### Queries de diagnostic

```sql
-- Derni√®res mises √† jour par table
SELECT 
  'trip_fact' as table_name,
  MAX(updated_at) as last_update,
  COUNT(*) as total_records
FROM ubear_gold.trip_fact
UNION ALL
SELECT 
  'dim_eater',
  MAX(effective_start_date),
  COUNT(*)
FROM ubear_gold.dim_eater
WHERE is_current = true;

-- V√©rifier la fra√Æcheur des donn√©es
SELECT 
  DATEDIFF(NOW(), MAX(event_time)) as days_old
FROM ubear_silver.trip_events_silver;
-- Doit √™tre < 1 jour

-- Statistiques SCD2
SELECT 
  is_current,
  COUNT(*) as count,
  AVG(version_number) as avg_version
FROM ubear_gold.dim_eater
GROUP BY is_current;
```

## üêõ Troubleshooting

### Pipeline Bronze ne d√©marre pas

**Probl√®me**: Erreur de connexion Kafka

**Solution**:
```bash
# V√©rifier la configuration Kafka
databricks pipelines get --pipeline-id <id> | grep kafka.bootstrap.servers

# Tester la connexion depuis un notebook
%python
from kafka import KafkaConsumer
consumer = KafkaConsumer(bootstrap_servers='kafka:9092')
print(consumer.topics())
```

### Expectations √©chouent dans Silver

**Probl√®me**: Trop de donn√©es invalides

**Solution**:
```python
# D√©sactiver temporairement les expectations strictes
# Dans silver_pipeline.py, remplacer:
# @dlt.expect_or_drop(...)
# par:
# @dlt.expect(...)  # Log seulement

# Puis analyser les violations
%sql
SELECT * FROM event_log('<pipeline_id>')
WHERE details:flow_progress.metrics.num_dropped_records > 0
```

### Job Gold prend trop de temps

**Probl√®me**: MERGE lent sur trip_fact

**Solution**:
```sql
-- Augmenter le nombre de workers dans le job
-- Ou optimiser la table avant MERGE
OPTIMIZE ubear_gold.trip_fact
ZORDER BY (trip_id, order_placed_at);

-- V√©rifier les statistiques
DESCRIBE DETAIL ubear_gold.trip_fact;
```

## üìö Ressources suppl√©mentaires

- üìñ [Architecture d√©taill√©e](ARCHITECTURE.md)
- üìò [Documentation compl√®te](README.md)
- üîß [Databricks DLT Docs](https://docs.databricks.com/delta-live-tables/)
- üí¨ Support: data-team@ubear.com

## ‚úÖ Checklist post-d√©ploiement

- [ ] Pipelines Bronze et Silver en mode continuous
- [ ] Job Gold schedul√© quotidien √† 2 AM UTC
- [ ] Alertes email configur√©es (on_failure)
- [ ] Monitoring dashboard cr√©√©
- [ ] Documentation √©quipe mise √† jour
- [ ] Acc√®s BI tools configur√©s (Tableau, Power BI)
- [ ] Tests de bout en bout r√©ussis
- [ ] Backup et disaster recovery plan d√©fini

üéâ **F√©licitations ! Votre Data Warehouse uBear est op√©rationnel !**
