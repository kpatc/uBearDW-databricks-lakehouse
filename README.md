# uBear Eats Data Warehouse - Databricks Lakehouse

![Architecture](https://img.shields.io/badge/Platform-Databricks-FF3621?logo=databricks)
![Delta Lake](https://img.shields.io/badge/Storage-Delta_Lake-00ADD8?logo=delta)
![Streaming](https://img.shields.io/badge/Streaming-Kafka-231F20?logo=apache-kafka)
![Python](https://img.shields.io/badge/Python-3.10-3776AB?logo=python)

Data Warehouse moderne pour uBear Eats construit sur Databricks Lakehouse avec architecture Medallion (Bronze-Silver-Gold).

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Structure du projet](#structure-du-projet)
- [Pipelines](#pipelines)
- [Configuration](#configuration)
- [DÃ©ploiement](#dÃ©ploiement)
- [DÃ©veloppement local](#dÃ©veloppement-local)
- [QualitÃ© des donnÃ©es](#qualitÃ©-des-donnÃ©es)

## ğŸ¯ Vue d'ensemble

Ce Data Warehouse centralise et transforme les donnÃ©es transactionnelles de uBear Eats (plateforme de livraison de nourriture) pour l'analyse et le reporting. Il couvre le parcours complet de la commande depuis le client jusqu'Ã  la livraison.

### Cas d'usage

- **Analyse des performances** : Suivi des mÃ©triques de livraison, temps de prÃ©paration, satisfaction client
- **Optimisation logistique** : Analyse des zones de livraison, performance des couriers
- **Business Intelligence** : Reporting des ventes, analyse des merchants, comportement clients
- **Data Science** : ModÃ¨les de prÃ©diction (temps de livraison, demand forecasting)

### DonnÃ©es sources

| Source | Description | Mode d'ingestion |
|--------|-------------|------------------|
| PostgreSQL `eater` | DonnÃ©es clients | CDC Streaming (Debezium) |
| PostgreSQL `merchant` | Restaurants/marchands | CDC Streaming (Debezium) |
| PostgreSQL `courier` | Livreurs | CDC Streaming (Debezium) |
| PostgreSQL `trip_events` | Ã‰vÃ©nements de commandes | CDC Streaming (Debezium) |

## ğŸ—ï¸ Architecture

### Architecture Medallion (Bronze â†’ Silver â†’ Gold)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SOURCES (PostgreSQL)                        â”‚
â”‚    Eater   â”‚   Merchant   â”‚   Courier   â”‚   Trip Events           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Debezium   â”‚ (CDC)
                    â”‚     Kafka    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BRONZE LAYER (Raw CDC)                         â”‚
â”‚  - trip_events_bronze    - eater_bronze                            â”‚
â”‚  - merchant_bronze       - courier_bronze                          â”‚
â”‚  Storage: Delta Lake | Mode: Streaming | DLT Pipeline              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SILVER LAYER (Cleaned & Validated)                â”‚
â”‚  - trip_events_silver    - eater_silver                            â”‚
â”‚  - merchant_silver       - courier_silver                          â”‚
â”‚  Storage: Delta Lake | Mode: Streaming | DLT Pipeline              â”‚
â”‚  Data Quality: Expectations, Deduplication, Parsing                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GOLD LAYER (Analytics Ready)                      â”‚
â”‚  DIMENSIONS (SCD Type 2):                                           â”‚
â”‚    - dim_eater          - dim_merchant      - dim_courier          â”‚
â”‚    - dim_date           - dim_time          - dim_location         â”‚
â”‚  FACT TABLE:                                                        â”‚
â”‚    - trip_fact (commandes & livraisons)                            â”‚
â”‚  Storage: Delta Lake | Mode: Batch | Databricks Notebook           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Reporting &   â”‚
                  â”‚   Analytics     â”‚
                  â”‚  (BI Tools)     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technologies utilisÃ©es

- **Platform**: Databricks (AWS/Azure/GCP)
- **Storage**: Delta Lake (ACID transactions, time travel, schema evolution)
- **Streaming**: Apache Kafka + Debezium CDC
- **Processing**: Apache Spark (PySpark)
- **Orchestration**: Databricks Workflows/Jobs
- **Data Quality**: Delta Live Tables Expectations
- **CI/CD**: Git integration avec Databricks Repos

## ğŸ“ Structure du projet

```
uBearDW-databricks-lakehouse/
â”œâ”€â”€ pipelines/                    # Pipelines DLT et notebooks
â”‚   â”œâ”€â”€ bronze_pipeline.py        # Ingestion CDC streaming (Kafka â†’ Bronze)
â”‚   â”œâ”€â”€ silver_pipeline.py        # Transformation et nettoyage (Bronze â†’ Silver)
â”‚   â””â”€â”€ gold_pipeline.py          # Dimensions SCD2 + Fact table (Silver â†’ Gold)
â”‚
â”œâ”€â”€ jobs/                         # Configurations Databricks Jobs
â”‚   â”œâ”€â”€ batch_job.json            # Job quotidien Gold layer (2 AM UTC)
â”‚   â””â”€â”€ streaming_job.json        # Job streaming continu (Bronze + Silver)
â”‚
â”œâ”€â”€ expectations/                 # RÃ¨gles de qualitÃ© donnÃ©es
â”‚   â””â”€â”€ data_quality.py           # Expectations DLT centralisÃ©es
â”‚
â”œâ”€â”€ utils/                        # Fonctions utilitaires rÃ©utilisables
â”‚   â””â”€â”€ transformations.py        # Transformations communes (SCD2, cleaning, etc.)
â”‚
â”œâ”€â”€ databricks_setup/             # Scripts de configuration initiale
â”‚   â””â”€â”€ 02_create_tables.sql      # DDL pour crÃ©ation tables Gold
â”‚
â”œâ”€â”€ local_stack/                  # Environnement local de dÃ©veloppement
â”‚   â”œâ”€â”€ docker-compose.yml        # Kafka + Debezium + PostgreSQL
â”‚   â”œâ”€â”€ initdb/init.sql           # SchÃ©ma PostgreSQL initial
â”‚   â”œâ”€â”€ generate_data.sh          # Script gÃ©nÃ©ration donnÃ©es de test
â”‚   â””â”€â”€ simulate_cdc.sh           # Simulation Ã©vÃ©nements CDC
â”‚
â”œâ”€â”€ README.md                     # Documentation principale
â””â”€â”€ requirements.txt              # DÃ©pendances Python
```

## ğŸš€ Pipelines

### 1. Bronze Pipeline (Streaming - DLT)

**Fichier**: `pipelines/bronze_pipeline.py`

Ingestion en temps rÃ©el des donnÃ©es CDC depuis Kafka vers Delta Lake.

**Tables crÃ©Ã©es**:
- `trip_events_bronze` - Ã‰vÃ©nements de commandes
- `eater_bronze` - Clients
- `merchant_bronze` - Restaurants
- `courier_bronze` - Livreurs

**CaractÃ©ristiques**:
- Mode: Streaming continu
- Source: Kafka (Debezium CDC envelope)
- Format: Delta Lake avec Change Data Feed activÃ©
- Watermark: 10 minutes sur `event_time`
- Expectations: Validation des clÃ©s primaires (NOT NULL)

**DÃ©marrage**:
```bash
# Via Databricks UI: Delta Live Tables â†’ Create Pipeline
# Ou via API:
databricks pipelines create --settings bronze_pipeline_config.json
```

### 2. Silver Pipeline (Streaming - DLT)

**Fichier**: `pipelines/silver_pipeline.py`

Transformation et nettoyage des donnÃ©es Bronze vers Silver.

**Transformations appliquÃ©es**:
- Parsing du payload JSON (trip_events)
- Nettoyage et normalisation (emails, adresses, postal codes)
- DÃ©duplication
- Calcul des partitions
- Validation de qualitÃ© (DLT Expectations)

**Expectations de qualitÃ©**:
- Validation emails (format, NOT NULL)
- Validation montants (â‰¥ 0)
- Validation ratings (1-5)
- Validation distances (< 100 miles)

### 3. Gold Pipeline (Batch - Notebook)

**Fichier**: `pipelines/gold_pipeline.py`

Transformation Silver vers Gold avec dimensions SCD2 et table de faits.

**Dimensions SCD Type 2** (historisation complÃ¨te):
- `dim_eater` - Historique des changements clients
- `dim_merchant` - Historique des changements restaurants
- `dim_courier` - Historique des changements livreurs

**Dimensions statiques**:
- `dim_date` - Calendrier (2020-2030)
- `dim_time` - Heures du jour avec pÃ©riodes (peak hours)
- `dim_location` - GÃ©ographie (Ã  implÃ©menter)

**Table de faits**:
- `trip_fact` - Commandes et livraisons (MERGE upsert sur `trip_id`)

**ExÃ©cution**: Job batch quotidien Ã  2 AM UTC

## âš™ï¸ Configuration

### Variables d'environnement Databricks

```python
# Configuration Ã  dÃ©finir dans Databricks Workflows
{
  "kafka.bootstrap.servers": "your-kafka-server:9092",
  "catalog": "ubear_catalog",
  "schema.bronze": "ubear_bronze",
  "schema.silver": "ubear_silver",
  "schema.gold": "ubear_gold"
}
```

### CrÃ©ation du catalogue et schÃ©mas

```sql
-- Dans Databricks SQL ou notebook
CREATE CATALOG IF NOT EXISTS ubear_catalog;

CREATE SCHEMA IF NOT EXISTS ubear_catalog.ubear_bronze
  COMMENT 'Raw CDC data from source systems';

CREATE SCHEMA IF NOT EXISTS ubear_catalog.ubear_silver
  COMMENT 'Cleaned and validated data';

CREATE SCHEMA IF NOT EXISTS ubear_catalog.ubear_gold
  COMMENT 'Analytics-ready dimensional model';
```

## ğŸ“¦ DÃ©ploiement

### PrÃ©requis

- Databricks Workspace (AWS/Azure/GCP)
- Kafka cluster avec Debezium CDC configurÃ©
- PostgreSQL source avec rÃ©plication logique activÃ©e
- Git repo connectÃ© Ã  Databricks Repos

### Ã‰tapes de dÃ©ploiement

#### 1. Configurer Databricks Repos

```bash
# Dans Databricks UI: Repos â†’ Add Repo
# URL: https://github.com/kpatc/uBearDW-databricks-lakehouse
# Branch: main
```

#### 2. CrÃ©er les pipelines DLT

**Bronze Pipeline**:
```bash
databricks pipelines create \
  --json '{
    "name": "ubear_bronze_streaming",
    "storage": "/mnt/datalake/ubear/dlt/bronze",
    "target": "ubear_bronze",
    "notebooks": ["/Repos/ubear-dw/pipelines/bronze_pipeline"],
    "configuration": {
      "kafka.bootstrap.servers": "kafka:9092"
    },
    "continuous": true
  }'
```

**Silver Pipeline**:
```bash
databricks pipelines create \
  --json '{
    "name": "ubear_silver_streaming",
    "storage": "/mnt/datalake/ubear/dlt/silver",
    "target": "ubear_silver",
    "notebooks": ["/Repos/ubear-dw/pipelines/silver_pipeline"],
    "continuous": true
  }'
```

#### 3. CrÃ©er le job batch Gold

```bash
databricks jobs create --json-file jobs/batch_job.json
```

#### 4. DÃ©marrer les pipelines

```bash
# DÃ©marrer Bronze streaming
databricks pipelines start --pipeline-id <bronze_pipeline_id>

# DÃ©marrer Silver streaming
databricks pipelines start --pipeline-id <silver_pipeline_id>

# Le job batch Gold est schedulÃ© quotidiennement (2 AM UTC)
```

## ğŸ› ï¸ DÃ©veloppement local

### Setup environnement local avec Docker

```bash
# DÃ©marrer PostgreSQL + Kafka + Debezium
cd local_stack
docker-compose up -d

# Attendre que les services dÃ©marrent (30-60 secondes)
sleep 30

# GÃ©nÃ©rer des donnÃ©es de test
./generate_data.sh

# Enregistrer le connecteur Debezium
./register_connector.sh

# Simuler des Ã©vÃ©nements CDC
./simulate_cdc.sh
```

### Tester les pipelines localement

Les pipelines DLT ne peuvent pas s'exÃ©cuter localement. Pour le dÃ©veloppement:

1. Utiliser Databricks Community Edition (gratuit)
2. Ou tester la logique PySpark dans des notebooks locaux
3. Utiliser `pytest` pour les fonctions dans `utils/`

```bash
# Installer les dÃ©pendances
pip install -r requirements.txt

# ExÃ©cuter les tests (si configurÃ©s)
pytest tests/
```

## ğŸ” QualitÃ© des donnÃ©es

### DLT Expectations

Le projet utilise Delta Live Tables Expectations pour garantir la qualitÃ©:

**Niveaux de validation**:
- `@dlt.expect()` - Log les violations (mÃ©triques)
- `@dlt.expect_or_drop()` - Rejette les enregistrements invalides
- `@dlt.expect_or_fail()` - Fait Ã©chouer le pipeline

**Exemple de rÃ¨gles** (voir `expectations/data_quality.py`):
```python
# Silver layer
@dlt.expect_or_drop("valid_email", "email IS NOT NULL AND email LIKE '%@%'")
@dlt.expect("valid_ratings", "eater_rating IS NULL OR (eater_rating >= 1 AND eater_rating <= 5)")

# Gold layer
@dlt.expect_or_fail("valid_foreign_keys", "eater_id IS NOT NULL AND merchant_id IS NOT NULL")
```

### Monitoring

AccÃ©dez aux mÃ©triques de qualitÃ© via:
- Databricks DLT Pipeline UI â†’ Data Quality tab
- Event Logs pour violations dÃ©taillÃ©es
- System tables: `system.dlt.<pipeline>.event_log`

## ğŸ“Š ModÃ¨le de donnÃ©es Gold

### Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_date    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  dim_eater   â”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”‚ dim_merchant â”‚
    â”‚   (SCD2)     â”‚       â”‚       â”‚   (SCD2)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  trip_fact   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  dim_time    â”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”‚ dim_courier  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚       â”‚   (SCD2)     â”‚
                           â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ dim_location â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tables principales

**trip_fact** (Faits):
- ClÃ©: `trip_id` (order_id)
- Mesures: montants (subtotal, delivery_fee, tip, total), mÃ©triques temporelles, ratings
- GranularitÃ©: Une ligne par commande/livraison
- Partitions: `date_partition`, `region_partition`

**Dimensions SCD2**:
- Historisation complÃ¨te des changements
- Colonnes SCD2: `effective_start_date`, `effective_end_date`, `is_current`, `version_number`, `row_hash`

## ğŸ¤ Contribution

Pour contribuer au projet:

1. Fork le repository
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ License

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¥ Contact

Data Engineering Team - data-team@ubear.com

Project Link: [https://github.com/kpatc/uBearDW-databricks-lakehouse](https://github.com/kpatc/uBearDW-databricks-lakehouse)

---

**Note**: Ce projet est un exemple d'architecture moderne de Data Warehouse sur Databricks Lakehouse. Adaptez-le selon vos besoins spÃ©cifiques.
