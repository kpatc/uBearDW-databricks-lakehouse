# uBear Data Warehouse - D√©ploiement GCP

Guide complet pour d√©ployer l'architecture Data Warehouse sur Google Cloud Platform avec Cloud SQL + Pub/Sub + Databricks.

## üèóÔ∏è Architecture

```
Cloud SQL PostgreSQL (db-f1-micro)
        ‚Üì (CDC via Debezium)
Google Cloud Pub/Sub (4 topics)
        ‚Üì (Streaming)
Databricks Delta Live Tables
        ‚Üì
Bronze ‚Üí Silver ‚Üí Gold (Lakehouse)
```

## üí∞ Co√ªts Estim√©s

| Service | Configuration | Co√ªt/mois | Free Tier |
|---------|--------------|-----------|-----------|
| Cloud SQL | db-f1-micro, 10GB | ~$10 | ‚úÖ $300 cr√©dit |
| Pub/Sub | <1M messages/mois | ~$5 | ‚úÖ 10GB gratuit/mois |
| Service Accounts | IAM | Gratuit | ‚úÖ |
| **Total** | | **~$15/mois** | **Gratuit avec cr√©dit** |

> üí° Avec le **Free Trial GCP ($300)**, tu as **20 mois gratuits** !

## üìã Pr√©requis

### 1. Compte GCP
```bash
# Cr√©er un compte: https://console.cloud.google.com
# Activer le Free Trial ($300 de cr√©dit)
```

### 2. Installer gcloud CLI
```bash
# Linux/Mac
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# Initialiser
gcloud init
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
```

### 3. Installer Terraform
```bash
# Linux
wget https://releases.hashicorp.com/terraform/1.6.0/terraform_1.6.0_linux_amd64.zip
unzip terraform_1.6.0_linux_amd64.zip
sudo mv terraform /usr/local/bin/

# V√©rifier
terraform version
```

### 4. Installer jq (optionnel)
```bash
sudo apt install jq  # Ubuntu/Debian
brew install jq      # Mac
```

## üöÄ D√©ploiement Automatique

### √âtape 1 : Cloner le repo
```bash
cd ~/Big\ Data\ Projects/BigProjectUbearDw
```

### √âtape 2 : Configurer GCP
```bash
# D√©finir ton projet
export GCP_PROJECT_ID="your-project-id"
gcloud config set project $GCP_PROJECT_ID

# Activer la facturation (requis m√™me pour free tier)
# https://console.cloud.google.com/billing
```

### √âtape 3 : D√©ployer l'infrastructure
```bash
# Rendre le script ex√©cutable
chmod +x deploy_gcp.sh

# Lancer le d√©ploiement
./deploy_gcp.sh
```

Le script va automatiquement :
1. ‚úÖ Activer les APIs GCP n√©cessaires
2. ‚úÖ Cr√©er Cloud SQL PostgreSQL (db-f1-micro)
3. ‚úÖ Cr√©er 5 topics Pub/Sub (eater, merchant, courier, trip_events, schema)
4. ‚úÖ Cr√©er 4 subscriptions Pub/Sub
5. ‚úÖ Cr√©er 2 Service Accounts (Debezium + Databricks)
6. ‚úÖ Configurer IAM roles
7. ‚úÖ Initialiser la base de donn√©es avec les tables
8. ‚úÖ Ins√©rer les donn√©es de test

## üîë R√©cup√©rer les Credentials

### Service Account Debezium
```bash
cd gcp_infrastructure

# Cr√©er la cl√© JSON
gcloud iam service-accounts keys create debezium-sa-key.json \
  --iam-account=$(terraform output -raw debezium_service_account)

# La cl√© est dans: ./debezium-sa-key.json
```

### Service Account Databricks
```bash
# Cr√©er la cl√© JSON
gcloud iam service-accounts keys create databricks-sa-key.json \
  --iam-account=$(terraform output -raw databricks_service_account)

# La cl√© est dans: ./databricks-sa-key.json
```

## üìù Configuration Databricks

### √âtape 1 : Upload Service Account Key

1. Dans Databricks Workspace, aller √† **Settings** ‚Üí **Compute**
2. Cr√©er un nouveau cluster ou √©diter l'existant
3. Dans **Advanced Options** ‚Üí **Spark Config**, ajouter :
```
gcp.project.id your-gcp-project-id
```

4. Dans **Environment Variables**, ajouter :
```
GOOGLE_APPLICATION_CREDENTIALS=/dbfs/secrets/gcp-sa-key.json
```

5. Uploader la cl√© :
```python
# Dans un notebook Databricks
dbutils.fs.mkdirs("/dbfs/secrets")
dbutils.fs.put("/dbfs/secrets/gcp-sa-key.json", """
COLLER LE CONTENU DE databricks-sa-key.json ICI
""", True)
```

### √âtape 2 : Cr√©er les DLT Pipelines

#### Bronze Pipeline (Pub/Sub ‚Üí Bronze)
```bash
# Dans Databricks UI:
# 1. Aller √† Delta Live Tables
# 2. Cliquer "Create Pipeline"
# 3. Configurer:

Name: ubear-bronze-pubsub-streaming
Notebook: /Repos/ubear-dw/pipelines/bronze_pipeline_pubsub
Storage Location: /tmp/ubear/dlt/bronze
Target: ubear_bronze
Continuous: ‚úÖ Enabled
Photon: ‚úÖ Enabled (si disponible)

# Advanced Configuration:
gcp.project.id: your-gcp-project-id
gcp.service.account.json.path: /dbfs/secrets/gcp-sa-key.json
```

#### Silver Pipeline (Bronze ‚Üí Silver)
```bash
# Utiliser le pipeline existant silver_pipeline_new.py
# Il lit depuis Bronze, pas besoin de changement
```

#### Gold Pipeline (Silver ‚Üí Gold)
```bash
# Utiliser le pipeline existant gold_pipeline_complete.py
# Job batch quotidien √† 2h UTC
```

### √âtape 3 : Installer les d√©pendances
```python
# Dans le cluster Databricks, installer:
# - google-cloud-pubsub (Maven: com.google.cloud:google-cloud-pubsub:1.120.0)
# - spark-pubsub-connector (Maven: com.google.cloud.spark:spark-pubsub:1.0.0)
```

## üß™ Tester l'Architecture

### Test 1 : V√©rifier Cloud SQL
```bash
# Se connecter √† Cloud SQL
gcloud sql connect ubear-postgres-dev --user=foodapp --database=foodapp

# V√©rifier les tables
\dt

# Compter les enregistrements
SELECT 'eater' as table, COUNT(*) FROM eater
UNION ALL SELECT 'merchant', COUNT(*) FROM merchant
UNION ALL SELECT 'courier', COUNT(*) FROM courier
UNION ALL SELECT 'trip_events', COUNT(*) FROM trip_events;

\q
```

### Test 2 : Ins√©rer des donn√©es et v√©rifier CDC
```bash
# Ins√©rer un nouvel eater
gcloud sql connect ubear-postgres-dev --user=foodapp --database=foodapp

INSERT INTO eater (eater_uuid, first_name, last_name, email, phone_number, 
                   address_line_1, city, state_province, postal_code, country, 
                   default_payment_method, is_active)
VALUES ('eater-uuid-999', 'Test', 'User', 'test.user@email.com', '+33699999999',
        '999 Test Street', 'Paris', 'Ile-de-France', '75001', 'France',
        'credit_card', true);
```

### Test 3 : V√©rifier Pub/Sub
```bash
# Lister les messages dans le topic eater
gcloud pubsub subscriptions pull ubear-eater-sub --limit=5 --auto-ack

# Tu devrais voir le nouveau eater !
```

### Test 4 : V√©rifier Bronze Table dans Databricks
```sql
-- Dans un notebook Databricks
SELECT * FROM ubear_bronze.eater_bronze 
WHERE email = 'test.user@email.com';
```

## üîÑ Mettre √† jour .env

```bash
# √âditer .env avec les nouvelles valeurs
cd ~/Big\ Data\ Projects/BigProjectUbearDw

# R√©cup√©rer les outputs Terraform
cd gcp_infrastructure
terraform output -json > ../terraform_outputs.json
cd ..

# Mettre √† jour .env
```

Copier dans `.env` :
```bash
# Cloud SQL
POSTGRES_HOST=<CLOUD_SQL_PUBLIC_IP>
POSTGRES_PORT=5432
POSTGRES_USER=foodapp
POSTGRES_PASSWORD=<FROM_DEPLOYMENT_SUMMARY>
POSTGRES_DB=foodapp

# GCP
GCP_PROJECT_ID=<YOUR_PROJECT_ID>
GCP_SERVICE_ACCOUNT_JSON=/dbfs/secrets/gcp-sa-key.json

# Pub/Sub Topics
PUBSUB_EATER_TOPIC=ubear-eater-cdc
PUBSUB_MERCHANT_TOPIC=ubear-merchant-cdc
PUBSUB_COURIER_TOPIC=ubear-courier-cdc
PUBSUB_TRIP_EVENTS_TOPIC=ubear-trip-events-cdc

# Pub/Sub Subscriptions
PUBSUB_EATER_SUB=projects/<PROJECT_ID>/subscriptions/ubear-eater-sub
PUBSUB_MERCHANT_SUB=projects/<PROJECT_ID>/subscriptions/ubear-merchant-sub
PUBSUB_COURIER_SUB=projects/<PROJECT_ID>/subscriptions/ubear-courier-sub
PUBSUB_TRIP_EVENTS_SUB=projects/<PROJECT_ID>/subscriptions/ubear-trip-events-sub

# Databricks (garder les valeurs existantes)
DATABRICKS_HOST=https://dbc-b9e469a8-62c4.cloud.databricks.com/
DATABRICKS_TOKEN=dapi39299043c10196e11c1b79fe86f5dbdc
```

## üìä Monitoring

### V√©rifier l'√©tat Pub/Sub
```bash
# Topics
gcloud pubsub topics list

# Subscriptions
gcloud pubsub subscriptions list

# M√©triques d'un topic
gcloud pubsub topics describe ubear-eater-cdc
```

### V√©rifier Cloud SQL
```bash
# √âtat de l'instance
gcloud sql instances describe ubear-postgres-dev

# Logs
gcloud sql operations list --instance=ubear-postgres-dev --limit=10
```

## üßπ Nettoyage (D√©truire l'infrastructure)

‚ö†Ô∏è **ATTENTION** : Cela supprime toutes les ressources et donn√©es !

```bash
cd gcp_infrastructure

# Voir ce qui sera d√©truit
terraform plan -destroy

# D√©truire
terraform destroy

# Confirmer avec 'yes'
```

## üêõ Troubleshooting

### Erreur: API not enabled
```bash
# Activer manuellement les APIs
gcloud services enable sqladmin.googleapis.com
gcloud services enable pubsub.googleapis.com
gcloud services enable iam.googleapis.com
```

### Erreur: Insufficient permissions
```bash
# Ajouter le r√¥le Owner √† ton compte
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
  --member="user:your-email@gmail.com" \
  --role="roles/owner"
```

### Erreur: Cloud SQL connection timeout
```bash
# V√©rifier que l'IP est autoris√©e
gcloud sql instances patch ubear-postgres-dev \
  --authorized-networks=0.0.0.0/0
```

### Databricks ne peut pas lire Pub/Sub
```bash
# V√©rifier les permissions du Service Account
gcloud projects get-iam-policy YOUR_PROJECT_ID \
  --flatten="bindings[].members" \
  --format="table(bindings.role)" \
  --filter="bindings.members:databricks-pubsub-reader@*"
```

## üìö Ressources

- [Cloud SQL Documentation](https://cloud.google.com/sql/docs)
- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)
- [Databricks GCP Integration](https://docs.databricks.com/administration-guide/cloud-configurations/gcp/index.html)
- [Terraform GCP Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs)

## üéØ Prochaines √âtapes

1. ‚úÖ Infrastructure GCP d√©ploy√©e
2. ‚è≥ Configurer Debezium pour Pub/Sub (voir `debezium_pubsub_connector.json`)
3. ‚è≥ Cr√©er les DLT Pipelines dans Databricks
4. ‚è≥ Tester le flux end-to-end
5. ‚è≥ Configurer les Jobs Databricks (Gold layer batch)
6. ‚è≥ Monitoring et alertes

---

**Besoin d'aide ?** Ouvre une issue sur GitHub ! üöÄ
