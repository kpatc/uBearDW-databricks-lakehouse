#!/bin/bash
# Script de dÃ©ploiement automatisÃ© pour uBear Data Warehouse
# Usage: ./deploy.sh [environment]
# Exemple: ./deploy.sh production

set -e  # Exit on error

ENVIRONMENT=${1:-development}
CONFIG_FILE="config.env"

echo "=========================================="
echo "uBear DW - DÃ©ploiement Databricks"
echo "Environnement: $ENVIRONMENT"
echo "=========================================="
echo ""

# VÃ©rifier que config.env existe
if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ Erreur: Fichier $CONFIG_FILE introuvable"
    echo "ğŸ“ Copiez config.env.example vers config.env et configurez vos paramÃ¨tres"
    exit 1
fi

# Charger la configuration
source $CONFIG_FILE

# VÃ©rifier Databricks CLI
if ! command -v databricks &> /dev/null; then
    echo "âŒ Databricks CLI non installÃ©"
    echo "ğŸ“¦ Installation: pip install databricks-cli"
    exit 1
fi

echo "âœ… Databricks CLI dÃ©tectÃ©"

# Configurer Databricks CLI
echo "ğŸ”§ Configuration Databricks..."
databricks configure --token <<EOF
$DATABRICKS_HOST
$DATABRICKS_TOKEN
EOF

echo "âœ… Configuration terminÃ©e"
echo ""

# Ã‰tape 1: CrÃ©er le catalogue et les schÃ©mas
echo "ğŸ“Š Ã‰tape 1/6: CrÃ©ation catalogue et schÃ©mas..."
databricks workspace import_dir databricks_setup /tmp/setup_scripts
databricks runs submit --json '{
  "run_name": "Setup Catalog and Schemas",
  "new_cluster": {
    "spark_version": "13.3.x-scala2.12",
    "node_type_id": "'$GOLD_CLUSTER_NODE_TYPE'",
    "num_workers": 1
  },
  "notebook_task": {
    "notebook_path": "/tmp/setup_scripts/02_create_tables",
    "base_parameters": {
      "catalog": "'$CATALOG_NAME'",
      "bronze_schema": "'$SCHEMA_BRONZE'",
      "silver_schema": "'$SCHEMA_SILVER'",
      "gold_schema": "'$SCHEMA_GOLD'"
    }
  }
}'

echo "âœ… Catalogue et schÃ©mas crÃ©Ã©s"
echo ""

# Ã‰tape 2: CrÃ©er le pipeline Bronze
echo "ğŸ¥‰ Ã‰tape 2/6: CrÃ©ation pipeline Bronze..."

cat > /tmp/bronze_pipeline_config.json <<EOF
{
  "name": "ubear_bronze_streaming_$ENVIRONMENT",
  "storage": "$BRONZE_STORAGE_PATH",
  "target": "$SCHEMA_BRONZE",
  "notebooks": [
    {
      "path": "/Repos/ubear-dw/pipelines/bronze_pipeline"
    }
  ],
  "configuration": {
    "kafka.bootstrap.servers": "$KAFKA_BOOTSTRAP_SERVERS",
    "kafka.topic.prefix": "$KAFKA_TOPIC_PREFIX",
    "pipelines.enableTrackHistory": "true"
  },
  "clusters": [
    {
      "label": "default",
      "autoscale": {
        "min_workers": $BRONZE_MIN_WORKERS,
        "max_workers": $BRONZE_MAX_WORKERS,
        "mode": "ENHANCED"
      }
    }
  ],
  "development": false,
  "continuous": $BRONZE_CONTINUOUS,
  "channel": "CURRENT",
  "edition": "ADVANCED"
}
EOF

BRONZE_PIPELINE_ID=$(databricks pipelines create --json-file /tmp/bronze_pipeline_config.json | jq -r '.pipeline_id')
echo "âœ… Pipeline Bronze crÃ©Ã©: $BRONZE_PIPELINE_ID"
echo ""

# Ã‰tape 3: CrÃ©er le pipeline Silver
echo "ğŸ¥ˆ Ã‰tape 3/6: CrÃ©ation pipeline Silver..."

cat > /tmp/silver_pipeline_config.json <<EOF
{
  "name": "ubear_silver_streaming_$ENVIRONMENT",
  "storage": "$SILVER_STORAGE_PATH",
  "target": "$SCHEMA_SILVER",
  "notebooks": [
    {
      "path": "/Repos/ubear-dw/pipelines/silver_pipeline"
    }
  ],
  "configuration": {
    "pipelines.enableTrackHistory": "true"
  },
  "clusters": [
    {
      "label": "default",
      "autoscale": {
        "min_workers": $SILVER_MIN_WORKERS,
        "max_workers": $SILVER_MAX_WORKERS,
        "mode": "ENHANCED"
      }
    }
  ],
  "development": false,
  "continuous": $SILVER_CONTINUOUS,
  "channel": "CURRENT",
  "edition": "ADVANCED",
  "photon": $SILVER_PHOTON_ENABLED
}
EOF

SILVER_PIPELINE_ID=$(databricks pipelines create --json-file /tmp/silver_pipeline_config.json | jq -r '.pipeline_id')
echo "âœ… Pipeline Silver crÃ©Ã©: $SILVER_PIPELINE_ID"
echo ""

# Ã‰tape 4: CrÃ©er le job Gold
echo "ğŸ¥‡ Ã‰tape 4/6: CrÃ©ation job Gold batch..."

# Substituer les variables dans batch_job.json
envsubst < jobs/batch_job.json > /tmp/batch_job_final.json

GOLD_JOB_ID=$(databricks jobs create --json-file /tmp/batch_job_final.json | jq -r '.job_id')
echo "âœ… Job Gold crÃ©Ã©: $GOLD_JOB_ID"
echo ""

# Ã‰tape 5: DÃ©marrer les pipelines
echo "â–¶ï¸  Ã‰tape 5/6: DÃ©marrage des pipelines..."

echo "DÃ©marrage pipeline Bronze..."
databricks pipelines start --pipeline-id $BRONZE_PIPELINE_ID

echo "Attente 30 secondes..."
sleep 30

echo "DÃ©marrage pipeline Silver..."
databricks pipelines start --pipeline-id $SILVER_PIPELINE_ID

echo "âœ… Pipelines dÃ©marrÃ©s"
echo ""

# Ã‰tape 6: RÃ©sumÃ©
echo "=========================================="
echo "âœ¨ DÃ©ploiement terminÃ© avec succÃ¨s !"
echo "=========================================="
echo ""
echo "ğŸ“ Informations de dÃ©ploiement:"
echo "   - Environnement: $ENVIRONMENT"
echo "   - Catalogue: $CATALOG_NAME"
echo "   - Pipeline Bronze ID: $BRONZE_PIPELINE_ID"
echo "   - Pipeline Silver ID: $SILVER_PIPELINE_ID"
echo "   - Job Gold ID: $GOLD_JOB_ID"
echo ""
echo "ğŸ”— Liens utiles:"
echo "   - Bronze Pipeline: $DATABRICKS_HOST/#joblist/pipelines/$BRONZE_PIPELINE_ID"
echo "   - Silver Pipeline: $DATABRICKS_HOST/#joblist/pipelines/$SILVER_PIPELINE_ID"
echo "   - Gold Job: $DATABRICKS_HOST/#job/$GOLD_JOB_ID"
echo ""
echo "ğŸ“Š Prochaines Ã©tapes:"
echo "   1. VÃ©rifier que les pipelines Bronze et Silver ingÃ¨rent des donnÃ©es"
echo "   2. ExÃ©cuter manuellement le job Gold pour la premiÃ¨re fois"
echo "   3. Configurer les alertes et le monitoring"
echo "   4. Donner accÃ¨s aux Ã©quipes Analytics/BI"
echo ""
echo "ğŸ‰ Votre Data Warehouse est prÃªt !"
echo ""

# Sauvegarder les IDs pour rÃ©fÃ©rence future
cat > deployment_info_$ENVIRONMENT.txt <<EOF
Deployment Date: $(date)
Environment: $ENVIRONMENT
Catalog: $CATALOG_NAME
Bronze Pipeline ID: $BRONZE_PIPELINE_ID
Silver Pipeline ID: $SILVER_PIPELINE_ID
Gold Job ID: $GOLD_JOB_ID
EOF

echo "ğŸ’¾ Informations sauvegardÃ©es dans: deployment_info_$ENVIRONMENT.txt"
