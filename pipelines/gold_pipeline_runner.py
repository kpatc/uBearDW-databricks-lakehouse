# Databricks notebook source
# MAGIC %md
# MAGIC # Gold Pipeline Runner - Wrapper pour Databricks Job
# MAGIC Ce notebook ex√©cute le gold_pipeline.py avec les param√®tres fournis par le Job

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. R√©cup√©rer les param√®tres du Job

# COMMAND ----------

# R√©cup√©rer les param√®tres avec gestion d'erreur
try:
    catalog = dbutils.widgets.get("catalog")
except:
    catalog = "ubear_catalog"

try:
    bronze_schema = dbutils.widgets.get("bronze_schema")
except:
    bronze_schema = "ubear_bronze"

try:
    silver_schema = dbutils.widgets.get("silver_schema")
except:
    silver_schema = "ubear_silver"

try:
    gold_schema = dbutils.widgets.get("gold_schema")
except:
    gold_schema = "ubear_gold"

print(f"üîß Configuration charg√©e:")
print(f"   - Catalog: {catalog}")
print(f"   - Bronze Schema: {bronze_schema}")
print(f"   - Silver Schema: {silver_schema}")
print(f"   - Gold Schema: {gold_schema}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. D√©finir la configuration Spark

# COMMAND ----------

spark.conf.set("catalog", catalog)
spark.conf.set("schema.bronze", bronze_schema)
spark.conf.set("schema.silver", silver_schema)
spark.conf.set("schema.gold", gold_schema)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Ex√©cuter le Gold Pipeline

# COMMAND ----------

# Importer le gold_pipeline
%run ./gold_pipeline

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Confirmation d'ex√©cution

# COMMAND ----------

print("‚úÖ Gold Pipeline ex√©cut√© avec succ√®s!")
print(f"   Timestamp: {spark.sql('SELECT current_timestamp() as ts').collect()[0][0]}")
